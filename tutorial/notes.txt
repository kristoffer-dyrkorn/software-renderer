
1. basic rasterizer

low-resolution setup so we can see what's going on.

we set indices in constructor - and have a separate array for vertices. (so they can be modified without impacting the structure of the triangle. will return to that later.)

note backface culling.
note bounding box
note integer-only operations.

note the edge functions, we compare against >= 0. (what to do with those ON edge? will return to that later.)

note lots of explicit calculations. full recalcuations for each pixel.


2. moar triangles, moar problems

f to toggle fill rule, space to toggle drawing of blue triangle

so: what to do with those ON edge? we get overlaps. steals performance and leads to visual errors (result depends on the sequence we draw triangles in)

why are we subtracting 1? and not some other number? it is just a tie breaker for the case that two different triangles have pixels that lie exactly ON the edge. the edge tests uses comparison with 0 - so adjusting by 1 is the smallest change that makes the pixels exactly on the edge belong to one tri and not the other.

and with that, we have now established the fill rule (it cannot be turned off/on by key pressesin the browser).

3. animation

set up an animation where the triangle rotates, we use requestAnimationFrame.

space to toogle drawing of the blue tri and p to pause the rotation

put the basis coordinates of the vertices centered in origo, then calculate a new set of rotated vertices, and place them back onto somewhere around the middle of the screen.

note although the vertices rotate, but the triangle edge data (the triangle structure) remains the same. this is the reason for the separation between the constructor (set up static vertex pointers for the triangles) and draw method (set up data spesific for that draw call in the parameter).

note the unsmooth rotation - we send integer coordinates to the rasterizer, and this makes the triangles jump. this can be improved!

4a. let's go continuous!

start by letting the rasterizer accept floating-point coordinates

expand the bounding box out to integer coordinates that lie outside the input

also, we need to support continuous coordinates in the edge tests themself. that means we no longer consider pixels to have integer coordinates, but to lie on continuous axes, with the center of a pixel set to integer coordinate + 0.5. thus the edge function inside the bounding box needs to be evaluated at x + 0.5 and y + 0.5. (the + 0.5 is just a choice/convention, we here have choosen to let the left edge of the first pixel be x=0.)

note the artifacts (the singel-pixel gap that moves between the triangles). the fill rule is correct and we use floating point numbers that should give us precision. what is wrong?

4b.

the rasterizer now runs on floating-point coordinates. unfortunately, floating-point calculations are not exact, and slight rounding errors (in the edge test, for example) might lead to visual artifacts - missed, or overdrawn, pixels. (here we observe gaps.) also, some floating-point operations take longer time than their corresponding integer versions.

what to do? make a hybrid. we multiply all coordinates by some fixed number, and round the result afterwards. this operation is effectively the same as subdividing the fractional part of a number into a fixed set of values - ie to do quantization. this hybrid number representaion is called fixed-point representation. it reduces the precision of the numbers we can represent, but at the same time it shifts all mathematical functions to be integer operations. so within the available precision we have, all operations will be exact. and as long as we multiply by a large enough number we will gain enough precision so that we keep the visual quality gained from using  floating point coordinates.
 
the number we choose to multiply by is some number 2^n. by doing that we can convert back to normal numbers very efficiently - we can bit-shift the integer value n positions to the right. this bit-shifting is the same as division without rounding. to support rounding we would need to add the value 0.5 (in fixed point representation) to the number before shifting.

so, we 1) keep the smooth movements, 2) avoid artifacts and 3) use very efficient integer calculations

which number 2^n is right? a large number would be nice (provide for high decimal resolution), but we cannot use too much space for the decimal part. 

we would like to keep the entire number (both integer part and decimal part) inside a 32-bit (signed) integer, thus the total amout of bits we can spend on the integer and decimal parts in total is 31. 

if we assume the x and y screen coordinates are in the range 0..2048, the integer part would fit inside 11 bits. however, when we calculate the determinant we multiply two fixed-point numbers, which means we need to have space for double the number of integer bits, ie 22 bits, inside our representation. under this assumption the maximum number of bits to allocate for the decimal part is 9.

ie we could divide each screen pixel into 9 subpixels.

normally, there are few noticeable improvements when you increase subpixel resolution beyond 4 - ie use 4 bits for the decimal part. and 4 bits of precision is also the GPU (hardware) implementation standard. so we have chosen that here. 4 bits means we multiply incoming floating point numbers by 16 (after rounding). this means we need to shift right by 4 to get back to a normal integer number. using 4 bits for the decimal part we have some extra headroom for calculations as well - we will not overflow so easily.

in short, by using fixed point numbers we create a larger resolution "virtual grid" of the pixels on screen, and perform precise integer calculations on that grid. moreover, the same quantization takes place for all the coordinates. that means, we essentially snap coordinates to the subpixel grid.

but - looking at the triangle rasterizer code now we see that we do a lot of operations per pixel. we need to optimize that.

one way to do so is to restructure our calculations so we move work up front (once per triangle draw invocation) so there is less work to do in the loop that is executed many times (once per pixel). from a quick profiling session we now seem to spend a lot of time calculating determinants and allocating memory.

on my machine, drawing this single triangle takes about 4.3 ms. we can go faster than that.

5. let's go incremental!

convert input to fixed point coords
setup up the determinant calculation to happen at top left pix in bounding box
keep the w value for left side of bbox
find dwdx and dwdy

loop inside bbox
- update wLeft with dwdy
- evaluate w
- update w with dwdx (the sub is just convention, dep on sub sequence, wanted code to look like as for dx)
- update w with dwdy

do you remember the fill rule - and tie breaker for pixels exactly on the edge? so far we have always tested for w >= 0. as we changed to fixed point arithmetic the snapping to subpixels means that we can test for > 0 instead. we can also rephrase the test for each component of w - we can bitwise OR the three numbers together before testing whether the resulting value is larger than zero.

triangle drawing now takes 0.26 ms, 6% of the time the previous version needed - or: a 16x speedup.

but....we have a bug.

6.

it turns out - when we calculate things incrementally, it is extremely important to start at the right place. (it is of course also important to make sure each incremental step is right - else errors will accumulate.)

when calculating the bounding box around the triangle we have so far (since we started using floating point coordinates for triangle vertices) just truncated the xmin and ymin values. these values are the input to the edge function evaluated at the top left point of the bounding box - ie the start value which all incremental calculations add onto later.

we need to round those coordinates instead - for the accumulated values to be correct.

the rounding is done as an addition before the right shift, and is performed once per triangle draw. so it will not affect the performance.

7.

some more optmizations, please?

bounding box surrounds the triangle, but is at times a lot larger than it. so, we evaluate w a lot more than we need to. could we improve on that?

a tighter bound can be found by directly calculating how many steps are needed before hitting the triangle edge on the left side.

      /*
      let dx = 0;
      if (wLeft[0] < 0 && dwdx[0] !== 0) {
        dx = Math.max(dx, Math.floor(-wLeft[0] / dwdx[0]));
      }
      if (wLeft[1] < 0 && dwdx[1] !== 0) {
        dx = Math.max(dx, Math.floor(-wLeft[1] / dwdx[1]));
      }
      if (wLeft[2] < 0 && dwdx[2] !== 0) {
        dx = Math.max(dx, Math.floor(-wLeft[2] / dwdx[2]));
      }

      tmpdwdx.copy(dwdx);
      tmpdwdx.scale(dx);
      wLeft.add(tmpdwdx);
      w.copy(wLeft);
      */

but.... this takes longer time! 0.30 ms vs 0.27 - the reason is most likely due to branching / branch misprediction and the operations themselves (division and floor). 

sometimes it is the case that the most efficient way (fewest steps) is *not* the fastest way. modern CPUs can very quickly run through loops of unconditional code - but as soon as code contains branches (if statements) the execution times will increase a lot.

---

and with that, we have built a correct and reasonably efficient triangle rasteizer.

---

read obj (random colors)
perspective
clipping

avoid mem allocation! 
def avoid not per pixel, but also avoid per triangle draw. ie: many triangles per object * 60 fps = VERY many allocations per sec. GC steals resources that can be used on drawing stuff


---

https://www.scratchapixel.com/lessons/3d-basic-rendering/rasterization-practical-implementation/rasterization-practical-implementation

https://gist.github.com/rygorous/9b793cd21d876da928bf4c7f3e625908

https://gist.github.com/rygorous/2486101